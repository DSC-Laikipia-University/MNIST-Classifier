{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/tanh.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The simplest unit in (deep) neural networks is a __linear operation (scaling(W) + offset(b))__ followed by an __activation function__. You had a linear operation in your latest model; the linear operation was the entire model. The activation function has the role of __concentrating__ the outputs of the preceding linear operation into a given __range__.\n",
    "\n",
    "- Suppose that you’re assigning a “good doggo” score to images. Pictures of dogs like retrievers and spaniels should have a high score; images of airplanes and garbage trucks should have a low score. Bear pictures should have a low-ish score too, though higher than garbage trucks. The problem is that you have to __define a high score__. Because you’ve got the entire range of __float32__ to work with, you can go pretty high. Even if you say “It’s a ten point scale,” sometimes your model is going to produce a score of 11 out of 10. Remember that under the hood, it’s all sum of __*wx+b matrix multiplications*__, which won’t naturally limit themselves to a specific range of outputs.\n",
    "- What you want to do is firmly constrain the output of your linear operation to a specific range so that the consumer of this output isn’t having to handle numerical inputs of puppies at 12/10, bears at -10, and garbage trucks at -1000.\n",
    "- One possibility is to cap the output values. Anything below zero is set to zero, and anything above 10 is set to 10. You use a simple activation function called [torch.nn.Hardtanh](https://pytorch.org/docs/stable/nn.html#hardtanh)\n",
    "- Another family of functions that works well is torch.nn.Sigmoid , which is 1 / (1 + e ** -x) , torch.tanh , and others that you’ll see in a moment. \n",
    "- These functions have a curve that asymptotically approaches zero or negative one as x goes to negative infinity, approaches one as x increases, and has a mostly constant slope at x == 0 . Conceptually, functions shaped this way work well, because it means that your neuron (which, again, is a linear function followed by an activation) will be sensitive to an area in the middle of your linear function’s output; everything else gets lumped up next tothe boundary values. \n",
    "- As you see in the figure below, a garbage truck gets a score of -0.97 , whereas bears, foxes, and wolves may end up somewhere in the -0.3 to 0.3 range.\n",
    "\n",
    "<img src='images/activation_function.png'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
