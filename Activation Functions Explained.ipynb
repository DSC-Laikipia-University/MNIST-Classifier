{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/tanh.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The simplest unit in (deep) neural networks is a __linear operation (scaling(W) + offset(b))__ followed by an __activation function__. You had a linear operation in your latest model; the linear operation was the entire model. The activation function has the role of __concentrating__ the outputs of the preceding linear operation into a given __range__.\n",
    "\n",
    "- Suppose that you’re assigning a “good doggo” score to images. Pictures of dogs like retrievers and spaniels should have a high score; images of airplanes and garbage trucks should have a low score. Bear pictures should have a low-ish score too, though higher than garbage trucks. The problem is that you have to __define a high score__. Because you’ve got the entire range of __float32__ to work with, you can go pretty high. Even if you say “It’s a ten point scale,” sometimes your model is going to produce a score of 11 out of 10. Remember that under the hood, it’s all sum of __*wx+b matrix multiplications*__, which won’t naturally limit themselves to a specific range of outputs.\n",
    "- What you want to do is firmly constrain the output of your linear operation to a specific range so that the consumer of this output isn’t having to handle numerical inputs of puppies at 12/10, bears at -10, and garbage trucks at -1000.\n",
    "- One possibility is to cap the output values. Anything below zero is set to zero, and anything above 10 is set to 10. You use a simple activation function called [torch.nn.Hardtanh](https://pytorch.org/docs/stable/nn.html#hardtanh)\n",
    "- Another family of functions that works well is torch.nn.Sigmoid , which is 1 / (1 + e ** -x) , torch.tanh , and others that you’ll see in a moment. \n",
    "- These functions have a curve that asymptotically approaches zero or negative one as x goes to negative infinity, approaches one as x increases, and has a mostly constant slope at x == 0 . Conceptually, functions shaped this way work well, because it means that your neuron (which, again, is a linear function followed by an activation) will be sensitive to an area in the middle of your linear function’s output; everything else gets lumped up next tothe boundary values. \n",
    "- As you see in the figure below, a garbage truck gets a score of -0.97 , whereas bears, foxes, and wolves may end up somewhere in the -0.3 to 0.3 range.\n",
    "\n",
    "<img src='images/activation_function.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Garbage trucks are flagged as “not dogs,” the good dog maps to “clearly a dog,” and the bear ends up somewhere in the middle. In code, you see the exact values:\n",
    "\n",
    "> import math\n",
    "\n",
    "> math.tanh(-2.2)\n",
    "<br/> -0.9757431300314515 \n",
    "<br/> __Garbage truck__\n",
    "\n",
    "> math.tanh(0.1)\n",
    "<br/> 0.09966799462495582\n",
    "<br/> __Bear__\n",
    "\n",
    "> math.tanh(2.5)\n",
    "<br/> 0.9866142981514303\n",
    "<br/> __Good doggo__\n",
    "\n",
    "\n",
    "- With the bear in the sensitive range, small changes to the bear result in a noticeable change in the result. You could swap from a grizzly to a polar bear (which has a vaguely more traditionally canine face) and see a jump up the Y axis as you slide toward the “very much a dog” end of the graph. Conversely, a koala bear would register as less doglike and would see a drop in the activated output. There isn’t much you could do to the garbage truck to make it register as doglike, though. \n",
    "- Even with drastic changes, you might see a shift only from -0.97 to -0.8 or so. Quite a few activation functions are available, some of which are pictured in figure below \n",
    "- In the first column, you see the continuous functions Tanh and Softplus ; the second column has “hard” versions of the activation functions to their left, Hardtanh and ReLU. ReLU (Rectified Linear Unit) deserves special note, as it is currently considered to be one of the best-performing general activation functions, as many state-of-the-art results have used it.\n",
    "\n",
    "<img src='images/the_functions.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Chapter 5 (Using a neural network to fit your data), page 101 of Deep Learning with PyTorch\n",
    "http://bit.ly/2QHPzmK"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
